#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ´åƒå®‰å…¨è¡›ç”Ÿæ³•ä»¤æ”¹æ­£è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - e-Gov APIç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
æ­£ç¢ºãªæ³•æ”¹æ­£æƒ…å ±ã‚’åé›†
"""

import requests
import json
import time
import re
import feedparser
import xml.etree.ElementTree as ET
import urllib.parse
from typing import Dict, List
from datetime import datetime, date, timedelta

class EgovAPIScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£ã®æ³•ä»¤ã‚’è­˜åˆ¥ã™ã‚‹æ­£è¦è¡¨ç¾
        self.safety_regex = re.compile(
            r"(åŠ´åƒå®‰å…¨è¡›ç”Ÿ|å®‰å…¨è¡›ç”Ÿ|åŠ´ç½|ã˜ã‚“è‚º|ä½œæ¥­ç’°å¢ƒ|"
            r"åŒ–å­¦ç‰©è³ª|ç‰¹å®šåŒ–å­¦ç‰©è³ª|æœ‰æ©Ÿæº¶å‰¤|ç²‰ã˜ã‚“|çŸ³ç¶¿|é‰›|"
            r"é…¸ç´ æ¬ ä¹|é›»é›¢æ”¾å°„ç·š|é«˜æ°—åœ§|ãƒœã‚¤ãƒ©ãƒ¼|ã‚¯ãƒ¬ãƒ¼ãƒ³|"
            r"åŠ´åƒåŸºæº–|åŠ´åƒå¥‘ç´„)"
        )
    
    def clean_html(self, text: str) -> str:
        """HTMLã‚¿ã‚°ã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ†ã‚­ã‚¹ãƒˆã«"""
        if not text:
            return ""
        # HTMLã‚¿ã‚°ã‚’å‰Šé™¤
        text = re.sub(r'<[^>]+>', ' ', text)
        # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def parse_feed(self, url: str):
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç¢ºå®Ÿã«å–å¾—"""
        try:
            r = requests.get(url, headers=self.headers, timeout=20)
            r.raise_for_status()
            feed = feedparser.parse(r.content)
            
            # å¤±æ•—ã‚’å¯è¦–åŒ–
            if getattr(feed, "bozo", 0):
                print(f"  [è­¦å‘Š] feedparser bozo: {feed.bozo_exception}")
            
            entries_count = len(getattr(feed, "entries", []))
            print(f"  å–å¾—: {entries_count}ä»¶")
            return feed
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return feedparser.FeedParserDict()
    
    def fetch_egov_updated_laws(self, lookback_days: int = 7) -> List[Dict]:
        """e-Gov æ›´æ–°æ³•ä»¤ä¸€è¦§API ã‹ã‚‰ç›´è¿‘Næ—¥åˆ†ã‚’å–å¾—"""
        print("\n[e-Gov API] æ›´æ–°æ³•ä»¤ä¸€è¦§")
        print("-" * 60)
        
        revisions = []
        
        for i in range(lookback_days + 1):
            d = (date.today() - timedelta(days=i)).strftime("%Y%m%d")
            url = f"https://laws.e-gov.go.jp/api/1/updatelawlists/{d}"
            
            try:
                r = requests.get(url, headers=self.headers, timeout=20)
                if r.status_code != 200:
                    continue
                
                root = ET.fromstring(r.text)
                code = root.findtext(".//Result/Code", default="1")
                if code != "0":
                    continue
                
                for info in root.findall(".//LawNameListInfo"):
                    law_name = info.findtext("LawName", default="").strip()
                    if not law_name or not self.safety_regex.search(law_name):
                        continue
                    
                    enforcement = info.findtext("EnforcementDate", default="").strip()
                    promulg = info.findtext("PromulgationDate", default="").strip()
                    amend_name = info.findtext("AmendName", default="").strip()
                    law_no = info.findtext("LawNo", default="").strip()
                    
                    # æ—¥ä»˜ã‚’YYYY-MM-DDå½¢å¼ã«å¤‰æ›
                    promulg_date = ""
                    if promulg and len(promulg) == 8:
                        promulg_date = f"{promulg[:4]}-{promulg[4:6]}-{promulg[6:]}"
                    
                    enf_date = ""
                    if enforcement and len(enforcement) == 8:
                        enf_date = f"{enforcement[:4]}-{enforcement[4:6]}-{enforcement[6:]}"
                    
                    # æ–½è¡Œæ—¥ãŒã‚ã‚Œã°ã€Œæ–½è¡Œäºˆå®šã€ã€ãªã‘ã‚Œã°ã€Œå…¬å¸ƒæ¸ˆã¿ã€
                    stage = "enforcement_scheduled" if enforcement else "promulgated"
                    if enforcement:
                        # æ–½è¡Œæ—¥ãŒéå»ãªã‚‰ã€Œæ–½è¡Œæ¸ˆã¿ã€
                        try:
                            enf_datetime = datetime.strptime(enforcement, "%Y%m%d")
                            if enf_datetime < datetime.now():
                                stage = "enforced"
                        except:
                            pass
                    
                    # æ³•ä»¤ç•ªå·ã‹ã‚‰è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
                    detail_url = "https://laws.e-gov.go.jp/"
                    if law_no:
                        # æ³•ä»¤ç•ªå·ã‚’URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šæ˜­å’Œå››åä¸ƒå¹´åŠ´åƒçœä»¤ç¬¬ä¸‰åäºŒå·ï¼‰
                        import urllib.parse
                        encoded_no = urllib.parse.quote(law_no)
                        detail_url = f"https://elaws.e-gov.go.jp/search/elawsSearch/elaws_search/lsg0500/viewContents?lawId={encoded_no}"
                    
                    revisions.append({
                        "title": law_name,
                        "lawName": law_name,
                        "description": f"æ”¹æ­£æ³•ä»¤: {amend_name}" if amend_name else "æ³•ä»¤æ›´æ–°",
                        "source": "e-Govæ›´æ–°æ³•ä»¤ä¸€è¦§API",
                        "stage": stage,
                        "publishedDate": f"{d[:4]}-{d[4:6]}-{d[6:8]}",
                        "officialUrl": detail_url,
                        "lawNo": law_no,
                        "promulgationDate": promulg_date,
                        "enforcementDate": enf_date,
                    })
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"  {d}: {e}")
                continue
        
        print(f"  åˆè¨ˆ: {len(revisions)}ä»¶ï¼ˆå®‰å…¨è¡›ç”Ÿãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰")
        return revisions
    
    def fetch_egov_pubcom_rss(self) -> List[Dict]:
        """e-Gov ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆåŠ´åƒã‚«ãƒ†ã‚´ãƒªï¼‰RSS"""
        print("\n[e-Gov] ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆRSSï¼ˆåŠ´åƒã‚«ãƒ†ã‚´ãƒªï¼‰")
        print("-" * 60)
        
        # åŠ´åƒã‚«ãƒ†ã‚´ãƒªã®æ„è¦‹å‹Ÿé›†RSS
        rss_url = "https://public-comment.e-gov.go.jp/rss/pcm_list_0000000046.xml"
        feed = self.parse_feed(rss_url)
        
        revisions = []
        for entry in feed.entries[:50]:
            title = entry.get("title", "")
            link = entry.get("link", "")
            published = entry.get("published", "")
            summary = entry.get("summary", "")
            
            # åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£ã®ã¿
            if self.safety_regex.search(title) or self.safety_regex.search(summary):
                revisions.append({
                    "title": title,
                    "url": link,
                    "publishedDate": self.parse_date(published),
                    "source": "e-Govãƒ‘ãƒ–ã‚³ãƒ¡ï¼ˆåŠ´åƒï¼‰",
                    "stage": "public_comment",
                    "description": self.clean_html(summary)[:300],
                })
        
        return revisions
    
    def fetch_kanpo_info(self) -> List[Dict]:
        """å›½ç«‹å°åˆ·å±€ å®˜å ±æƒ…å ±ã‚’å–å¾—"""
        print("\n[å®˜å ±] å›½ç«‹å°åˆ·å±€")
        print("-" * 60)
        
        revisions = []
        
        # å®˜å ±æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã®RSS
        kanpo_rss_url = "https://kanpou.npb.go.jp/rss/kanpou.rss"
        
        try:
            feed = self.parse_feed(kanpo_rss_url)
            
            for entry in feed.entries[:100]:  # æœ€æ–°100ä»¶
                title = entry.get("title", "")
                link = entry.get("link", "")
                published = entry.get("published", "")
                description = entry.get("description", "")
                
                # åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£ã‚’ãƒ•ã‚£ãƒ«ã‚¿
                full_text = title + " " + description
                if not self.safety_regex.search(full_text):
                    continue
                
                # å…¬å¸ƒãƒ»æ”¹æ­£ã«é–¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
                if any(keyword in full_text for keyword in ['å…¬å¸ƒ', 'æ”¹æ­£', 'çœä»¤', 'è¦å‰‡', 'å‘Šç¤º']):
                    revisions.append({
                        "title": title,
                        "url": link,
                        "publishedDate": self.parse_date(published),
                        "source": "å®˜å ±",
                        "stage": "promulgated",
                        "description": self.clean_html(description)[:300],
                    })
            
            print(f"  å–å¾—: {len(revisions)}ä»¶ï¼ˆåŠ´åƒå®‰å…¨è¡›ç”Ÿãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰")
            return revisions
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_specific_topics(self) -> List[Dict]:
        """é‡è¦ãƒˆãƒ”ãƒƒã‚¯ï¼ˆæ‰‹å‹•ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        print("\n[æ‰‹å‹•ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³] é‡è¦ãƒˆãƒ”ãƒƒã‚¯")
        print("-" * 60)
        
        topics = [
            {
                "title": "é«˜å¹´é½¢åŠ´åƒè€…ã®å®‰å…¨è¡›ç”Ÿå¯¾ç­–ï¼ˆã‚¨ã‚¤ã‚¸ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰",
                "url": "https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/anzen/newpage_00007.html",
                "stage": "enforced",
                "description": "60æ­³ä»¥ä¸Šã®é«˜å¹´é½¢åŠ´åƒè€…ã«å¯¾ã™ã‚‹å®‰å…¨è¡›ç”Ÿå¯¾ç­–ã®å®Ÿæ–½ã‚’æ¨é€²",
                "lawName": "åŠ´åƒå®‰å…¨è¡›ç”Ÿæ³•",
            },
            {
                "title": "åŒ–å­¦ç‰©è³ªè¦åˆ¶ã®è¦‹ç›´ã—ï¼ˆç¬¬2æ®µéšãƒ»ç´„850ç‰©è³ªè¿½åŠ ï¼‰",
                "url": "https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000099121_00005.html",
                "stage": "enforcement_scheduled",
                "description": "2026å¹´4æœˆã‹ã‚‰ç´„850ç‰©è³ªã‚’è¿½åŠ ã—ã€åˆè¨ˆç´„2,450ç‰©è³ªã«å¯¾ã—ã¦ãƒªã‚¹ã‚¯ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆç¾©å‹™åŒ–",
                "lawName": "ç‰¹å®šåŒ–å­¦ç‰©è³ªéšœå®³äºˆé˜²è¦å‰‡",
                "enforcementDate": "2026-04-01",
            },
            {
                "title": "çŸ³ç¶¿éšœå®³äºˆé˜²è¦å‰‡ã®æ”¹æ­£",
                "url": "https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/gyousei/anzen/sekimen/index.html",
                "stage": "enforced",
                "description": "çŸ³ç¶¿ï¼ˆã‚¢ã‚¹ãƒ™ã‚¹ãƒˆï¼‰ã®äº‹å‰èª¿æŸ»ãƒ»å ±å‘Šã®ç¾©å‹™åŒ–",
                "lawName": "çŸ³ç¶¿éšœå®³äºˆé˜²è¦å‰‡",
            },
        ]
        
        revisions = []
        for topic in topics:
            revisions.append({
                "title": topic["title"],
                "officialUrl": topic["url"],
                "description": topic["description"],
                "source": "é‡è¦ãƒˆãƒ”ãƒƒã‚¯",
                "stage": topic["stage"],
                "lawName": topic.get("lawName", "åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£"),
                "enforcementDate": topic.get("enforcementDate", ""),
            })
            print(f"  âœ“ {topic['title']}")
        
        return revisions
    
    def parse_date(self, date_string: str) -> str:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            if date_string:
                dt = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')
                return dt.strftime('%Y-%m-%d')
        except:
            pass
        return datetime.now().strftime('%Y-%m-%d')
    
    def collect_all_data(self) -> List[Dict]:
        """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print("=" * 60)
        print("e-Gov APIç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹")
        print("=" * 60)
        
        all_data = []
        
        # 1. e-Gov æ›´æ–°æ³•ä»¤ä¸€è¦§APIï¼ˆæœ€ã‚‚é‡è¦ï¼‰
        updated_laws = self.fetch_egov_updated_laws(lookback_days=7)
        all_data.extend(updated_laws)
        
        # 2. e-Gov ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆRSS
        pubcom_data = self.fetch_egov_pubcom_rss()
        all_data.extend(pubcom_data)
        time.sleep(1)
        
        # 3. å®˜å ±æƒ…å ±
        kanpo_data = self.fetch_kanpo_info()
        all_data.extend(kanpo_data)
        time.sleep(1)
        
        # 4. é‡è¦ãƒˆãƒ”ãƒƒã‚¯ï¼ˆæ‰‹å‹•ï¼‰
        topics_data = self.fetch_specific_topics()
        all_data.extend(topics_data)
        
        print("\n" + "=" * 60)
        print(f"åˆè¨ˆåé›†: {len(all_data)}ä»¶")
        print("=" * 60)
        
        return all_data
    
    def generate_revision_list(self, raw_data: List[Dict]) -> List[Dict]:
        """åé›†ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢"""
        revisions = []
        seen_titles = set()
        
        for idx, item in enumerate(raw_data, 1):
            title = item.get('title', '').strip()
            
            # é‡è¤‡é™¤å¤–
            if not title or title in seen_titles:
                continue
            seen_titles.add(title)
            
            revision = {
                'id': idx,
                'lawName': item.get('lawName', 'åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£æ³•ä»¤'),
                'title': title,
                'stage': item.get('stage', 'consideration'),
                'description': item.get('description', '')[:300],
                'officialUrl': item.get('url') or item.get('officialUrl', ''),
                'source': item.get('source', 'e-Gov'),
                'collectedDate': datetime.now().strftime('%Y-%m-%d')
            }
            
            # æ—¥ä»˜æƒ…å ±
            for date_field in ['publishedDate', 'promulgationDate', 'enforcementDate']:
                date_value = item.get(date_field)
                if date_value and date_value != "":
                    # YYYYMMDDå½¢å¼ã®å ´åˆã¯YYYY-MM-DDã«å¤‰æ›
                    if len(str(date_value)) == 8 and str(date_value).isdigit():
                        date_value = f"{date_value[:4]}-{date_value[4:6]}-{date_value[6:]}"
                    revision[date_field] = date_value
            
            revisions.append(revision)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ä»¶æ•°ã‚’è¡¨ç¤º
        stage_counts = {}
        for rev in revisions:
            stage = rev['stage']
            stage_counts[stage] = stage_counts.get(stage, 0) + 1
        
        print("\nğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ä»¶æ•°:")
        stage_names = {
            'consideration': 'æ¤œè¨æ®µéš',
            'public_comment': 'ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ',
            'deliberation': 'å›½ä¼šå¯©è­°ä¸­',
            'promulgated': 'å…¬å¸ƒæ¸ˆã¿',
            'enforcement_scheduled': 'æ–½è¡Œäºˆå®š',
            'enforced': 'æ–½è¡Œæ¸ˆã¿'
        }
        for stage, count in sorted(stage_counts.items()):
            print(f"  {stage_names.get(stage, stage)}: {count}ä»¶")
        
        return revisions


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = EgovAPIScraper()
    
    # ãƒ‡ãƒ¼ã‚¿åé›†
    raw_data = scraper.collect_all_data()
    
    # æ”¹æ­£ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    revisions = scraper.generate_revision_list(raw_data)
    
    # JSONã«ä¿å­˜
    with open('revisions_list.json', 'w', encoding='utf-8') as f:
        json.dump(revisions, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å‡¦ç†å®Œäº†: {len(revisions)}ä»¶ã®æ”¹æ­£æƒ…å ±ã‚’ç”Ÿæˆ")
    print("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: revisions_list.json")


if __name__ == "__main__":
    main()
