#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ´åƒå®‰å…¨è¡›ç”Ÿæ³•ä»¤æ”¹æ­£è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - API/RSSç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ã‚ˆã‚Šç¢ºå®Ÿã§æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿åé›†
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from typing import Dict, List
from datetime import datetime
import re
import feedparser

class APIRSSScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.base_url = 'https://www.mhlw.go.jp'
        
    def fetch_mhlw_rss(self) -> List[Dict]:
        """åšç”ŸåŠ´åƒçœã®æ–°ç€æƒ…å ±RSSã‹ã‚‰å–å¾—"""
        rss_url = 'https://www.mhlw.go.jp/stf/news.rdf'
        
        try:
            print("\n[RSS] åšç”ŸåŠ´åƒçœ æ–°ç€æƒ…å ±")
            print("-" * 60)
            
            feed = feedparser.parse(rss_url)
            
            revisions = []
            for entry in feed.entries[:50]:  # æœ€æ–°50ä»¶
                title = entry.title
                link = entry.link
                published = entry.get('published', '')
                
                # åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
                keywords = [
                    'åŠ´åƒå®‰å…¨è¡›ç”Ÿ', 'åŠ´åƒåŸºæº–', 'å®‰å…¨è¡›ç”Ÿ', 'åŠ´ç½',
                    'æ”¹æ­£', 'çœä»¤', 'è¦å‰‡', 'æ³•å¾‹', 'æ–½è¡Œ', 'å…¬å¸ƒ',
                    'ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ', 'åŒ–å­¦ç‰©è³ª', 'é«˜å¹´é½¢',
                    'ã‚¹ãƒˆãƒ¬ã‚¹ãƒã‚§ãƒƒã‚¯', 'ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹', 'å¥åº·è¨ºæ–­'
                ]
                
                if any(keyword in title for keyword in keywords):
                    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                    date = self.parse_date(published)
                    
                    revisions.append({
                        'title': title,
                        'url': link,
                        'publishedDate': date,
                        'source': 'RSS',
                        'description': entry.get('summary', '')[:300]
                    })
            
            print(f"  å–å¾—: {len(revisions)}ä»¶")
            return revisions
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_egov_public_comments(self) -> List[Dict]:
        """e-Gov ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆHTMLç‰ˆï¼‰"""
        url = 'https://public-comment.e-gov.go.jp/servlet/Public'
        
        try:
            print("\n[e-Gov] ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ")
            print("-" * 60)
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            revisions = []
            
            # åŠ´åƒé–¢é€£ã®ãƒ‘ãƒ–ã‚³ãƒ¡ã‚’æ¢ã™
            for link in soup.find_all('a', href=True):
                text = link.get_text(strip=True)
                
                keywords = ['åŠ´åƒå®‰å…¨è¡›ç”Ÿ', 'åŠ´åƒåŸºæº–', 'åŠ´ç½', 'å®‰å…¨è¡›ç”Ÿ']
                if any(keyword in text for keyword in keywords):
                    href = link['href']
                    if not href.startswith('http'):
                        href = 'https://public-comment.e-gov.go.jp' + href
                    
                    revisions.append({
                        'title': text,
                        'url': href,
                        'source': 'ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ',
                        'stage': 'public_comment'
                    })
            
            print(f"  å–å¾—: {len(revisions)}ä»¶")
            return revisions
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_kanpo_info(self) -> List[Dict]:
        """å®˜å ±æƒ…å ±ï¼ˆç°¡æ˜“ç‰ˆ - åšç”ŸåŠ´åƒçœãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰"""
        url = 'https://www.mhlw.go.jp/hourei/index.html'
        
        try:
            print("\n[å®˜å ±] æ³•ä»¤å…¬å¸ƒæƒ…å ±")
            print("-" * 60)
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            revisions = []
            
            # æ³•ä»¤ã®å…¬å¸ƒæƒ…å ±ã‚’æ¢ã™
            for link in soup.find_all('a', href=True):
                text = link.get_text(strip=True)
                
                if 'å…¬å¸ƒ' in text or 'æ³•å¾‹' in text or 'çœä»¤' in text:
                    keywords = ['åŠ´åƒå®‰å…¨è¡›ç”Ÿ', 'åŠ´åƒåŸºæº–', 'å®‰å…¨è¡›ç”Ÿ']
                    if any(keyword in text for keyword in keywords):
                        href = link['href']
                        if not href.startswith('http'):
                            href = self.base_url + href
                        
                        revisions.append({
                            'title': text,
                            'url': href,
                            'source': 'å®˜å ±',
                            'stage': 'promulgated'
                        })
            
            print(f"  å–å¾—: {len(revisions)}ä»¶")
            return revisions
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_specific_topics(self) -> List[Dict]:
        """é‡è¦ãƒˆãƒ”ãƒƒã‚¯ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥å–å¾—"""
        topics = [
            {
                'url': 'https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/anzen/newpage_00007.html',
                'title': 'é«˜å¹´é½¢åŠ´åƒè€…ã®å®‰å…¨è¡›ç”Ÿå¯¾ç­–',
                'stage': 'enforced',
                'description': '70æ­³ä»¥ä¸Šã®åŠ´åƒè€…ã«å¯¾ã™ã‚‹ç‰¹åˆ¥ãªå®‰å…¨è¡›ç”Ÿå¯¾ç­–'
            },
            {
                'url': 'https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000099121_00005.html',
                'title': 'åŒ–å­¦ç‰©è³ªè¦åˆ¶ã®è¦‹ç›´ã—',
                'stage': 'enforcement_scheduled',
                'description': 'ç´„2,450ç‰©è³ªã«å¯¾ã™ã‚‹ãƒªã‚¹ã‚¯ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆç¾©å‹™åŒ–'
            },
            {
                'url': 'https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/anzen/anzen-roudou.html',
                'title': 'åŠ´åƒç½å®³é˜²æ­¢å¯¾ç­–',
                'stage': 'consideration',
                'description': 'åŠ´åƒç½å®³ã‚’é˜²æ­¢ã™ã‚‹ãŸã‚ã®ç·åˆçš„ãªå¯¾ç­–'
            },
            {
                'url': 'https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/health.html',
                'title': 'å¥åº·ç¢ºä¿å¯¾ç­–',
                'stage': 'enforced',
                'description': 'ã‚¹ãƒˆãƒ¬ã‚¹ãƒã‚§ãƒƒã‚¯åˆ¶åº¦ç­‰ã®å¥åº·ç¢ºä¿å¯¾ç­–'
            }
        ]
        
        print("\n[é‡è¦ãƒˆãƒ”ãƒƒã‚¯] å€‹åˆ¥ãƒšãƒ¼ã‚¸")
        print("-" * 60)
        
        revisions = []
        for topic in topics:
            revisions.append({
                'title': topic['title'],
                'url': topic['url'],
                'description': topic['description'],
                'source': 'é‡è¦ãƒˆãƒ”ãƒƒã‚¯',
                'stage': topic['stage']
            })
            print(f"  âœ“ {topic['title']}")
        
        return revisions
    
    def parse_date(self, date_string: str) -> str:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # RSSã®æ—¥ä»˜å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            if date_string:
                dt = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')
                return dt.strftime('%Y-%m-%d')
        except:
            pass
        
        return datetime.now().strftime('%Y-%m-%d')
    
    def determine_stage_from_title(self, title: str, description: str = '') -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ”¹æ­£æ®µéšã‚’æ¨å®š"""
        text = title + ' ' + description
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šï¼ˆå„ªå…ˆåº¦é †ï¼‰
        if 'ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ' in text or 'ãƒ‘ãƒ–ã‚³ãƒ¡' in text or 'æ„è¦‹å‹Ÿé›†' in text:
            return 'public_comment'
        elif 'æ–½è¡Œã—ã¾ã—ãŸ' in text or 'æ–½è¡Œã•ã‚Œã¾ã—ãŸ' in text or 'é©ç”¨é–‹å§‹' in text:
            return 'enforced'
        elif 'æ–½è¡Œäºˆå®š' in text or 'æ–½è¡Œã•ã‚Œã‚‹äºˆå®š' in text or 'æ–½è¡Œæ—¥' in text:
            return 'enforcement_scheduled'
        elif 'å…¬å¸ƒ' in text and ('ã—ã¾ã—ãŸ' in text or 'ã•ã‚Œã¾ã—ãŸ' in text):
            return 'promulgated'
        elif 'å›½ä¼š' in text or 'æ³•æ¡ˆ' in text or 'å¯©è­°ä¸­' in text:
            return 'deliberation'
        elif 'æ¤œè¨' in text or 'ç ”ç©¶ä¼š' in text or 'å¯©è­°ä¼š' in text:
            return 'consideration'
        else:
            return 'consideration'
    
    def collect_all_data(self) -> List[Dict]:
        """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print("=" * 60)
        print("API/RSSç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹")
        print("=" * 60)
        
        all_data = []
        
        # 1. RSS
        rss_data = self.fetch_mhlw_rss()
        all_data.extend(rss_data)
        time.sleep(1)
        
        # 2. ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ
        egov_data = self.fetch_egov_public_comments()
        all_data.extend(egov_data)
        time.sleep(1)
        
        # 3. å®˜å ±æƒ…å ±
        kanpo_data = self.fetch_kanpo_info()
        all_data.extend(kanpo_data)
        time.sleep(1)
        
        # 4. é‡è¦ãƒˆãƒ”ãƒƒã‚¯
        topics_data = self.fetch_specific_topics()
        all_data.extend(topics_data)
        
        print("\n" + "=" * 60)
        print(f"åˆè¨ˆåé›†: {len(all_data)}ä»¶")
        print("=" * 60)
        
        return all_data
    
    def generate_revision_list(self, raw_data: List[Dict]) -> List[Dict]:
        """åé›†ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢"""
        revisions = []
        seen_titles = set()
        
        for idx, item in enumerate(raw_data, 1):
            title = item.get('title', '').strip()
            
            # é‡è¤‡é™¤å¤–ãƒ»çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«é™¤å¤–
            if not title or title in seen_titles or len(title) < 10:
                continue
            seen_titles.add(title)
            
            # æ”¹æ­£æ®µéšã‚’æ±ºå®š
            stage = item.get('stage')
            if not stage:
                stage = self.determine_stage_from_title(
                    title, 
                    item.get('description', '')
                )
            
            revision = {
                'id': idx,
                'lawName': self.extract_law_name(title),
                'title': title,
                'stage': stage,
                'description': item.get('description', title)[:300],
                'officialUrl': item.get('url', ''),
                'source': item.get('source', 'åšç”ŸåŠ´åƒçœ'),
                'collectedDate': datetime.now().strftime('%Y-%m-%d')
            }
            
            # æ—¥ä»˜æƒ…å ±
            if 'publishedDate' in item:
                revision['publishedDate'] = item['publishedDate']
            
            revisions.append(revision)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸ã”ã¨ã«ä»¶æ•°ã‚’è¡¨ç¤º
        stage_counts = {}
        for rev in revisions:
            stage = rev['stage']
            stage_counts[stage] = stage_counts.get(stage, 0) + 1
        
        print("\nğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ä»¶æ•°:")
        stage_names = {
            'consideration': 'æ¤œè¨æ®µéš',
            'public_comment': 'ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ',
            'deliberation': 'å›½ä¼šå¯©è­°ä¸­',
            'promulgated': 'å…¬å¸ƒæ¸ˆã¿',
            'enforcement_scheduled': 'æ–½è¡Œäºˆå®š',
            'enforced': 'æ–½è¡Œæ¸ˆã¿'
        }
        for stage, count in stage_counts.items():
            print(f"  {stage_names.get(stage, stage)}: {count}ä»¶")
        
        return revisions
    
    def extract_law_name(self, title: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ³•ä»¤åã‚’æŠ½å‡º"""
        law_keywords = [
            'åŠ´åƒå®‰å…¨è¡›ç”Ÿæ³•', 'åŠ´åƒåŸºæº–æ³•', 'åŠ´åƒå¥‘ç´„æ³•', 'ã˜ã‚“è‚ºæ³•',
            'ç‰¹å®šåŒ–å­¦ç‰©è³ª', 'çŸ³ç¶¿', 'ãƒœã‚¤ãƒ©ãƒ¼', 'ã‚¯ãƒ¬ãƒ¼ãƒ³',
            'æœ‰æ©Ÿæº¶å‰¤', 'ç²‰ã˜ã‚“', 'é«˜æ°—åœ§', 'é›»é›¢æ”¾å°„ç·š'
        ]
        
        for keyword in law_keywords:
            if keyword in title:
                return keyword
        
        return 'åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£æ³•ä»¤'


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    scraper = APIRSSScraper()
    
    # ãƒ‡ãƒ¼ã‚¿åé›†
    raw_data = scraper.collect_all_data()
    
    # æ”¹æ­£ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    revisions = scraper.generate_revision_list(raw_data)
    
    # JSONã«ä¿å­˜
    with open('revisions_list.json', 'w', encoding='utf-8') as f:
        json.dump(revisions, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å‡¦ç†å®Œäº†: {len(revisions)}ä»¶ã®æ”¹æ­£æƒ…å ±ã‚’ç”Ÿæˆ")
    print("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: revisions_list.json")


if __name__ == "__main__":
    main()
