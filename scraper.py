#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ´åƒå®‰å…¨è¡›ç”Ÿæ³•ä»¤æ”¹æ­£è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆ
"""

import requests
import json
import time
import re
import feedparser
import xml.etree.ElementTree as ET
import urllib.parse
from typing import Dict, List
from datetime import datetime, date, timedelta

class CompleteScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£ã®æ³•ä»¤ã‚’è­˜åˆ¥ã™ã‚‹æ­£è¦è¡¨ç¾
        self.safety_regex = re.compile(
            r"(åŠ´åƒå®‰å…¨è¡›ç”Ÿ|å®‰å…¨è¡›ç”Ÿ|åŠ´ç½|ã˜ã‚“è‚º|ä½œæ¥­ç’°å¢ƒ|"
            r"åŒ–å­¦ç‰©è³ª|ç‰¹å®šåŒ–å­¦ç‰©è³ª|æœ‰æ©Ÿæº¶å‰¤|ç²‰ã˜ã‚“|çŸ³ç¶¿|é‰›|"
            r"é…¸ç´ æ¬ ä¹|é›»é›¢æ”¾å°„ç·š|é«˜æ°—åœ§|ãƒœã‚¤ãƒ©ãƒ¼|ã‚¯ãƒ¬ãƒ¼ãƒ³|"
            r"åŠ´åƒåŸºæº–|åŠ´åƒå¥‘ç´„)"
        )
    
    def clean_html(self, text: str) -> str:
        """HTMLã‚¿ã‚°ã‚’å‰Šé™¤"""
        if not text:
            return ""
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def parse_feed(self, url: str):
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç¢ºå®Ÿã«å–å¾—"""
        try:
            r = requests.get(url, headers=self.headers, timeout=20)
            r.raise_for_status()
            feed = feedparser.parse(r.content)
            
            if getattr(feed, "bozo", 0):
                print(f"  [è­¦å‘Š] feedparser bozo: {feed.bozo_exception}")
            
            entries_count = len(getattr(feed, "entries", []))
            print(f"  å–å¾—: {entries_count}ä»¶")
            return feed
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return feedparser.FeedParserDict()
    
    def fetch_egov_law_api_v2(self, lookback_days: int = 30) -> List[Dict]:
        """e-Gov æ³•ä»¤API Version 2 - æ›´æ–°æ³•ä»¤ä¸€è¦§"""
        print("\n[e-Gov API v2] æ›´æ–°æ³•ä»¤ä¸€è¦§")
        print("-" * 60)
        
        revisions = []
        
        for i in range(lookback_days + 1):
            d = (date.today() - timedelta(days=i)).strftime("%Y%m%d")
            url = f"https://laws.e-gov.go.jp/api/1/updatelawlists/{d}"
            
            try:
                r = requests.get(url, headers=self.headers, timeout=20)
                if r.status_code != 200:
                    continue
                
                root = ET.fromstring(r.text)
                code = root.findtext(".//Result/Code", default="1")
                if code != "0":
                    continue
                
                for info in root.findall(".//LawNameListInfo"):
                    law_name = info.findtext("LawName", default="").strip()
                    if not law_name or not self.safety_regex.search(law_name):
                        continue
                    
                    law_no = info.findtext("LawNo", default="").strip()
                    law_id = info.findtext("LawId", default="").strip()
                    enforcement = info.findtext("EnforcementDate", default="").strip()
                    promulg = info.findtext("PromulgationDate", default="").strip()
                    amend_name = info.findtext("AmendName", default="").strip()
                    
                    # æ—¥ä»˜å¤‰æ›
                    promulg_date = f"{promulg[:4]}-{promulg[4:6]}-{promulg[6:]}" if len(promulg) == 8 else ""
                    enf_date = f"{enforcement[:4]}-{enforcement[4:6]}-{enforcement[6:]}" if len(enforcement) == 8 else ""
                    
                    # æ–½è¡ŒçŠ¶æ…‹ã‚’åˆ¤å®š
                    stage = "promulgated"
                    if enforcement:
                        try:
                            enf_datetime = datetime.strptime(enforcement, "%Y%m%d")
                            stage = "enforced" if enf_datetime < datetime.now() else "enforcement_scheduled"
                        except:
                            stage = "enforcement_scheduled"
                    
                    # æ³•ä»¤è©³ç´°URLï¼ˆVersion 2å¯¾å¿œï¼‰
                    detail_url = f"https://elaws.e-gov.go.jp/document?lawid={law_id}" if law_id else "https://laws.e-gov.go.jp/"
                    
                    revisions.append({
                        "title": law_name,
                        "lawName": law_name,
                        "lawNo": law_no,
                        "lawId": law_id,
                        "description": f"æ”¹æ­£æ³•ä»¤: {amend_name}" if amend_name else "æ³•ä»¤æ›´æ–°",
                        "source": "e-Govæ³•ä»¤API v2",
                        "stage": stage,
                        "publishedDate": f"{d[:4]}-{d[4:6]}-{d[6:8]}",
                        "promulgationDate": promulg_date,
                        "enforcementDate": enf_date,
                        "officialUrl": detail_url,
                    })
                
                time.sleep(0.5)
                
            except Exception as e:
                continue
        
        print(f"  åˆè¨ˆ: {len(revisions)}ä»¶")
        return revisions
    
    def fetch_egov_pubcom_rss(self) -> List[Dict]:
        """e-Gov ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆåŠ´åƒã‚«ãƒ†ã‚´ãƒªï¼‰"""
        print("\n[e-Gov] ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆRSS")
        print("-" * 60)
        
        rss_url = "https://public-comment.e-gov.go.jp/rss/pcm_list_0000000046.xml"
        feed = self.parse_feed(rss_url)
        
        revisions = []
        for entry in feed.entries[:50]:
            title = entry.get("title", "")
            link = entry.get("link", "")
            published = entry.get("published", "")
            summary = entry.get("summary", "")
            
            if self.safety_regex.search(title) or self.safety_regex.search(summary):
                revisions.append({
                    "title": title,
                    "officialUrl": link,
                    "publishedDate": self.parse_date(published),
                    "source": "e-Govãƒ‘ãƒ–ã‚³ãƒ¡",
                    "stage": "public_comment",
                    "description": self.clean_html(summary)[:300],
                })
        
        return revisions
    
    def fetch_kanpo_gov(self) -> List[Dict]:
        """å®˜å ±ç™ºè¡Œã‚µã‚¤ãƒˆï¼ˆå†…é–£åºœï¼‰"""
        print("\n[å®˜å ±] å®˜å ±ç™ºè¡Œã‚µã‚¤ãƒˆ")
        print("-" * 60)
        
        revisions = []
        
        # å®˜å ±RSS
        kanpo_rss = "https://kanpou.npb.go.jp/rss/kanpou.rss"
        feed = self.parse_feed(kanpo_rss)
        
        for entry in feed.entries[:100]:
            title = entry.get("title", "")
            link = entry.get("link", "")
            published = entry.get("published", "")
            description = entry.get("description", "")
            
            full_text = title + " " + description
            if not self.safety_regex.search(full_text):
                continue
            
            if any(kw in full_text for kw in ['å…¬å¸ƒ', 'æ”¹æ­£', 'çœä»¤', 'è¦å‰‡', 'å‘Šç¤º']):
                revisions.append({
                    "title": title,
                    "officialUrl": link,
                    "publishedDate": self.parse_date(published),
                    "source": "å®˜å ±",
                    "stage": "promulgated",
                    "description": self.clean_html(description)[:300],
                })
        
        print(f"  åˆè¨ˆ: {len(revisions)}ä»¶")
        return revisions
    
    def fetch_anzeninfo_mhlw(self) -> List[Dict]:
        """è·å ´ã®ã‚ã‚“ãœã‚“ã‚µã‚¤ãƒˆ - æ³•ä»¤æ”¹æ­£ä¸€è¦§"""
        print("\n[åšåŠ´çœ] è·å ´ã®ã‚ã‚“ãœã‚“ã‚µã‚¤ãƒˆ")
        print("-" * 60)
        
        url = "https://anzeninfo.mhlw.go.jp/information/horei.html"
        
        try:
            r = requests.get(url, headers=self.headers, timeout=20)
            r.encoding = 'utf-8'
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(r.text, 'html.parser')
            
            revisions = []
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ³•ä»¤æ”¹æ­£æƒ…å ±ã‚’æŠ½å‡º
            for table in soup.find_all('table'):
                for row in table.find_all('tr'):
                    cells = row.find_all(['td', 'th'])
                    if len(cells) < 2:
                        continue
                    
                    text = ' '.join([cell.get_text(strip=True) for cell in cells])
                    if not self.safety_regex.search(text):
                        continue
                    
                    # ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    link_tag = row.find('a')
                    link = ""
                    if link_tag and link_tag.get('href'):
                        href = link_tag['href']
                        if href.startswith('http'):
                            link = href
                        else:
                            link = f"https://anzeninfo.mhlw.go.jp{href}"
                    
                    title = cells[0].get_text(strip=True) if cells else text[:100]
                    
                    revisions.append({
                        "title": title,
                        "officialUrl": link or url,
                        "source": "è·å ´ã®ã‚ã‚“ãœã‚“ã‚µã‚¤ãƒˆ",
                        "stage": "consideration",
                        "description": text[:300],
                    })
            
            print(f"  åˆè¨ˆ: {len(revisions)}ä»¶")
            return revisions
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_mhlw_special_pages(self) -> List[Dict]:
        """åšç”ŸåŠ´åƒçœ ç‰¹è¨­ãƒšãƒ¼ã‚¸ï¼ˆæ‰‹å‹•ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        print("\n[åšåŠ´çœ] ç‰¹è¨­ãƒšãƒ¼ã‚¸")
        print("-" * 60)
        
        topics = [
            {
                "title": "é«˜å¹´é½¢åŠ´åƒè€…ã®å®‰å…¨è¡›ç”Ÿå¯¾ç­–ï¼ˆã‚¨ã‚¤ã‚¸ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰",
                "url": "https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/anzen/newpage_00007.html",
                "stage": "enforced",
                "description": "60æ­³ä»¥ä¸Šã®é«˜å¹´é½¢åŠ´åƒè€…ã«å¯¾ã™ã‚‹å®‰å…¨è¡›ç”Ÿå¯¾ç­–ã®å®Ÿæ–½ã‚’æ¨é€²",
            },
            {
                "title": "åŒ–å­¦ç‰©è³ªè¦åˆ¶ã®è¦‹ç›´ã—ï¼ˆç¬¬2æ®µéšãƒ»ç´„850ç‰©è³ªè¿½åŠ ï¼‰",
                "url": "https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000099121_00005.html",
                "stage": "enforcement_scheduled",
                "description": "2026å¹´4æœˆã‹ã‚‰ç´„850ç‰©è³ªã‚’è¿½åŠ ã—ã€åˆè¨ˆç´„2,450ç‰©è³ªã«å¯¾ã—ã¦ãƒªã‚¹ã‚¯ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆç¾©å‹™åŒ–",
                "enforcementDate": "2026-04-01",
            },
            {
                "title": "çŸ³ç¶¿éšœå®³äºˆé˜²è¦å‰‡ã®æ”¹æ­£",
                "url": "https://www.mhlw.go.jp/stf/seisakunitsuite/bunya/koyou_roudou/roudoukijun/gyousei/anzen/sekimen/index.html",
                "stage": "enforced",
                "description": "çŸ³ç¶¿ï¼ˆã‚¢ã‚¹ãƒ™ã‚¹ãƒˆï¼‰ã®äº‹å‰èª¿æŸ»ãƒ»å ±å‘Šã®ç¾©å‹™åŒ–",
            },
        ]
        
        revisions = []
        for topic in topics:
            revisions.append({
                "title": topic["title"],
                "officialUrl": topic["url"],
                "description": topic["description"],
                "source": "åšåŠ´çœç‰¹è¨­ãƒšãƒ¼ã‚¸",
                "stage": topic["stage"],
                "enforcementDate": topic.get("enforcementDate", ""),
            })
            print(f"  âœ“ {topic['title']}")
        
        return revisions
    
    def parse_date(self, date_string: str) -> str:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            if date_string:
                dt = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')
                return dt.strftime('%Y-%m-%d')
        except:
            pass
        return datetime.now().strftime('%Y-%m-%d')
    
    def collect_all_data(self) -> List[Dict]:
        """ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        print("=" * 60)
        print("å®Œå…¨ç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹")
        print("=" * 60)
        
        all_data = []
        
        # 1. e-Gov æ³•ä»¤API v2
        egov_data = self.fetch_egov_law_api_v2(lookback_days=30)
        all_data.extend(egov_data)
        
        # 2. e-Gov ãƒ‘ãƒ–ã‚³ãƒ¡
        pubcom_data = self.fetch_egov_pubcom_rss()
        all_data.extend(pubcom_data)
        time.sleep(1)
        
        # 3. å®˜å ±
        kanpo_data = self.fetch_kanpo_gov()
        all_data.extend(kanpo_data)
        time.sleep(1)
        
        # 4. è·å ´ã®ã‚ã‚“ãœã‚“ã‚µã‚¤ãƒˆ
        anzen_data = self.fetch_anzeninfo_mhlw()
        all_data.extend(anzen_data)
        time.sleep(1)
        
        # 5. åšåŠ´çœç‰¹è¨­ãƒšãƒ¼ã‚¸
        mhlw_data = self.fetch_mhlw_special_pages()
        all_data.extend(mhlw_data)
        
        print("\n" + "=" * 60)
        print(f"åˆè¨ˆåé›†: {len(all_data)}ä»¶")
        print("=" * 60)
        
        return all_data
    
    def generate_revision_list(self, raw_data: List[Dict]) -> List[Dict]:
        """åé›†ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢"""
        revisions = []
        seen_titles = set()
        
        for idx, item in enumerate(raw_data, 1):
            title = item.get('title', '').strip()
            
            if not title or title in seen_titles:
                continue
            seen_titles.add(title)
            
            revision = {
                'id': idx,
                'lawName': item.get('lawName', 'åŠ´åƒå®‰å…¨è¡›ç”Ÿé–¢é€£æ³•ä»¤'),
                'title': title,
                'stage': item.get('stage', 'consideration'),
                'description': item.get('description', '')[:300],
                'officialUrl': item.get('officialUrl', ''),
                'source': item.get('source', ''),
                'collectedDate': datetime.now().strftime('%Y-%m-%d')
            }
            
            # æ—¥ä»˜æƒ…å ±
            for date_field in ['publishedDate', 'promulgationDate', 'enforcementDate']:
                date_value = item.get(date_field)
                if date_value and date_value != "":
                    if len(str(date_value)) == 8 and str(date_value).isdigit():
                        date_value = f"{date_value[:4]}-{date_value[4:6]}-{date_value[6:]}"
                    revision[date_field] = date_value
            
            # æ³•ä»¤ID
            if item.get('lawId'):
                revision['lawId'] = item['lawId']
            if item.get('lawNo'):
                revision['lawNo'] = item['lawNo']
            
            revisions.append(revision)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ä»¶æ•°
        stage_counts = {}
        for rev in revisions:
            stage = rev['stage']
            stage_counts[stage] = stage_counts.get(stage, 0) + 1
        
        print("\nğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ä»¶æ•°:")
        stage_names = {
            'consideration': 'æ¤œè¨æ®µéš',
            'public_comment': 'ãƒ‘ãƒ–ãƒªãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆ',
            'deliberation': 'å›½ä¼šå¯©è­°ä¸­',
            'promulgated': 'å…¬å¸ƒæ¸ˆã¿',
            'enforcement_scheduled': 'æ–½è¡Œäºˆå®š',
            'enforced': 'æ–½è¡Œæ¸ˆã¿'
        }
        for stage, count in sorted(stage_counts.items()):
            print(f"  {stage_names.get(stage, stage)}: {count}ä»¶")
        
        return revisions


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    scraper = CompleteScraper()
    
    # ãƒ‡ãƒ¼ã‚¿åé›†
    raw_data = scraper.collect_all_data()
    
    # æ”¹æ­£ãƒªã‚¹ãƒˆç”Ÿæˆ
    revisions = scraper.generate_revision_list(raw_data)
    
    # JSONã«ä¿å­˜
    with open('revisions_list.json', 'w', encoding='utf-8') as f:
        json.dump(revisions, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å‡¦ç†å®Œäº†: {len(revisions)}ä»¶")
    print("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: revisions_list.json")


if __name__ == "__main__":
    main()
